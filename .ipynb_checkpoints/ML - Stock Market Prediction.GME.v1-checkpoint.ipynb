{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lauch this with C:\\<dir>\\jupyter-lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"GME\"\n",
    "data_point = \"Open\" \n",
    "training_percent = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-02-13</td>\n",
       "      <td>2.40625</td>\n",
       "      <td>2.51500</td>\n",
       "      <td>2.38125</td>\n",
       "      <td>2.51250</td>\n",
       "      <td>1.691666</td>\n",
       "      <td>76216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002-02-14</td>\n",
       "      <td>2.54375</td>\n",
       "      <td>2.54875</td>\n",
       "      <td>2.48125</td>\n",
       "      <td>2.50000</td>\n",
       "      <td>1.683250</td>\n",
       "      <td>11021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002-02-15</td>\n",
       "      <td>2.50000</td>\n",
       "      <td>2.50625</td>\n",
       "      <td>2.46250</td>\n",
       "      <td>2.48750</td>\n",
       "      <td>1.674834</td>\n",
       "      <td>8389600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002-02-19</td>\n",
       "      <td>2.47500</td>\n",
       "      <td>2.47500</td>\n",
       "      <td>2.34375</td>\n",
       "      <td>2.38750</td>\n",
       "      <td>1.607504</td>\n",
       "      <td>7410400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002-02-20</td>\n",
       "      <td>2.40000</td>\n",
       "      <td>2.46875</td>\n",
       "      <td>2.38125</td>\n",
       "      <td>2.46875</td>\n",
       "      <td>1.662210</td>\n",
       "      <td>6892800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4693</th>\n",
       "      <td>2020-10-05</td>\n",
       "      <td>2.36000</td>\n",
       "      <td>2.39750</td>\n",
       "      <td>2.31250</td>\n",
       "      <td>2.36500</td>\n",
       "      <td>2.365000</td>\n",
       "      <td>11220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4694</th>\n",
       "      <td>2020-10-06</td>\n",
       "      <td>2.39000</td>\n",
       "      <td>2.46000</td>\n",
       "      <td>2.27500</td>\n",
       "      <td>2.28250</td>\n",
       "      <td>2.282500</td>\n",
       "      <td>18141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4695</th>\n",
       "      <td>2020-10-07</td>\n",
       "      <td>2.30750</td>\n",
       "      <td>2.39000</td>\n",
       "      <td>2.29250</td>\n",
       "      <td>2.34000</td>\n",
       "      <td>2.340000</td>\n",
       "      <td>13234400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4696</th>\n",
       "      <td>2020-10-08</td>\n",
       "      <td>2.38500</td>\n",
       "      <td>3.41000</td>\n",
       "      <td>2.29750</td>\n",
       "      <td>3.37250</td>\n",
       "      <td>3.372500</td>\n",
       "      <td>305814400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4697</th>\n",
       "      <td>2020-10-09</td>\n",
       "      <td>3.20750</td>\n",
       "      <td>3.70000</td>\n",
       "      <td>2.97500</td>\n",
       "      <td>3.00500</td>\n",
       "      <td>3.005000</td>\n",
       "      <td>308611200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4698 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date     Open     High      Low    Close  Adj Close     Volume\n",
       "0     2002-02-13  2.40625  2.51500  2.38125  2.51250   1.691666   76216000\n",
       "1     2002-02-14  2.54375  2.54875  2.48125  2.50000   1.683250   11021600\n",
       "2     2002-02-15  2.50000  2.50625  2.46250  2.48750   1.674834    8389600\n",
       "3     2002-02-19  2.47500  2.47500  2.34375  2.38750   1.607504    7410400\n",
       "4     2002-02-20  2.40000  2.46875  2.38125  2.46875   1.662210    6892800\n",
       "...          ...      ...      ...      ...      ...        ...        ...\n",
       "4693  2020-10-05  2.36000  2.39750  2.31250  2.36500   2.365000   11220000\n",
       "4694  2020-10-06  2.39000  2.46000  2.27500  2.28250   2.282500   18141600\n",
       "4695  2020-10-07  2.30750  2.39000  2.29250  2.34000   2.340000   13234400\n",
       "4696  2020-10-08  2.38500  3.41000  2.29750  3.37250   3.372500  305814400\n",
       "4697  2020-10-09  3.20750  3.70000  2.97500  3.00500   3.005000  308611200\n",
       "\n",
       "[4698 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "# Part 1 - Data Preprocessing\n",
    "\n",
    "# Importing the training and test sets\n",
    "file = \".\\\\Data\\\\\" + ticker + \".csv\"\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "train_count = int(len(df) * (training_percent/100))\n",
    "test_count = int(len(df) * ((100 - training_percent)/100))\n",
    "\n",
    "dataset_train = df.head(train_count)\n",
    "dataset_test = df.tail(test_count)\n",
    "\n",
    "# '.values' need the 2nd Column Opening Price as a Numpy array (not vector)\n",
    "# '1:2' is used because the upper bound is ignored\n",
    "training_set = dataset_train.iloc[:, 1:2].values\n",
    "\n",
    "# Feature Scaling\n",
    "# Use Normalization (versus Standardization) for RNNs with Sigmoid Activation Functions\n",
    "# 'MinMaxScalar' is a Normalization Library\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 'feature_range = (0,1)' makes sure that training data is scaled to have values between 0 and 1\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "\n",
    "# Creating a data structure with 60 timesteps (look back 60 days) and 1 output\n",
    "# This tells the RNN what to remember (Number of timesteps) when predicting the next Stock Price\n",
    "# The wrong number of timesteps can lead to Overfitting or bogus results\n",
    "# 'x_train' Input with 60 previous days' stock prices\n",
    "X_train = []\n",
    "# 'y_train' Output with next day's stock price\n",
    "y_train = []\n",
    "\n",
    "# 'rg' is the length of the training data.  this ensures we loop through all of the training data\n",
    "rg = training_set_scaled.shape[0]\n",
    "\n",
    "for i in range(60, rg):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshaping (add more dimensions)\n",
    "# This lets you add more indicators that may potentially have corelation with Stock Prices\n",
    "# Keras RNNs expects an input shape (Batch Size, Timesteps, input_dim)\n",
    "# '.shape[0]' is the number of Rows (Batch Size)\n",
    "# '.shape[1]' is the number of Columns (timesteps)\n",
    "# 'input_dim' is the number of factors that may affect stock prices\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Show the dataset we're working with\n",
    "display(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "145/145 [==============================] - 13s 28ms/step - loss: 0.0068\n",
      "Epoch 2/100\n",
      "145/145 [==============================] - 4s 26ms/step - loss: 0.0029\n",
      "Epoch 3/100\n",
      "145/145 [==============================] - 4s 26ms/step - loss: 0.0026\n",
      "Epoch 4/100\n",
      "145/145 [==============================] - 4s 28ms/step - loss: 0.0022\n",
      "Epoch 5/100\n",
      "145/145 [==============================] - 4s 26ms/step - loss: 0.0020\n",
      "Epoch 6/100\n",
      "145/145 [==============================] - 4s 25ms/step - loss: 0.0020\n",
      "Epoch 7/100\n",
      "145/145 [==============================] - 4s 27ms/step - loss: 0.0018\n",
      "Epoch 8/100\n",
      "145/145 [==============================] - 4s 26ms/step - loss: 0.0016\n",
      "Epoch 9/100\n",
      "145/145 [==============================] - 4s 26ms/step - loss: 0.0016\n",
      "Epoch 10/100\n",
      "145/145 [==============================] - 4s 26ms/step - loss: 0.0015\n",
      "Epoch 11/100\n",
      "145/145 [==============================] - 4s 26ms/step - loss: 0.0014\n",
      "Epoch 12/100\n",
      "145/145 [==============================] - 4s 26ms/step - loss: 0.0013\n",
      "Epoch 13/100\n",
      "145/145 [==============================] - 4s 30ms/step - loss: 0.0013\n",
      "Epoch 14/100\n",
      "145/145 [==============================] - 5s 31ms/step - loss: 0.0012\n",
      "Epoch 15/100\n",
      "145/145 [==============================] - 5s 34ms/step - loss: 0.0012\n",
      "Epoch 16/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 0.0011\n",
      "Epoch 17/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 0.0011\n",
      "Epoch 18/100\n",
      "145/145 [==============================] - 4s 30ms/step - loss: 9.8060e-04\n",
      "Epoch 19/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 0.0010\n",
      "Epoch 20/100\n",
      "145/145 [==============================] - 5s 33ms/step - loss: 0.0010\n",
      "Epoch 21/100\n",
      "145/145 [==============================] - 5s 34ms/step - loss: 0.0010\n",
      "Epoch 22/100\n",
      "145/145 [==============================] - 5s 33ms/step - loss: 9.1032e-04\n",
      "Epoch 23/100\n",
      "145/145 [==============================] - 5s 36ms/step - loss: 9.8158e-04\n",
      "Epoch 24/100\n",
      "145/145 [==============================] - 5s 35ms/step - loss: 9.3292e-04\n",
      "Epoch 25/100\n",
      "145/145 [==============================] - 5s 31ms/step - loss: 9.8357e-04\n",
      "Epoch 26/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 8.9617e-04\n",
      "Epoch 27/100\n",
      "145/145 [==============================] - 5s 31ms/step - loss: 8.4721e-04\n",
      "Epoch 28/100\n",
      "145/145 [==============================] - 5s 31ms/step - loss: 8.5195e-04\n",
      "Epoch 29/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 8.2598e-04\n",
      "Epoch 30/100\n",
      "145/145 [==============================] - 4s 31ms/step - loss: 7.9356e-04\n",
      "Epoch 31/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 7.7851e-04\n",
      "Epoch 32/100\n",
      "145/145 [==============================] - 5s 31ms/step - loss: 8.2072e-04\n",
      "Epoch 33/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 7.2501e-04\n",
      "Epoch 34/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 8.3878e-04\n",
      "Epoch 35/100\n",
      "145/145 [==============================] - 5s 31ms/step - loss: 7.8835e-04\n",
      "Epoch 36/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 8.4426e-04\n",
      "Epoch 37/100\n",
      "145/145 [==============================] - 5s 33ms/step - loss: 7.4988e-04\n",
      "Epoch 38/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 7.1757e-04\n",
      "Epoch 39/100\n",
      "145/145 [==============================] - 5s 33ms/step - loss: 7.8320e-04\n",
      "Epoch 40/100\n",
      "145/145 [==============================] - 5s 31ms/step - loss: 7.4353e-04\n",
      "Epoch 41/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 6.8533e-04\n",
      "Epoch 42/100\n",
      "145/145 [==============================] - 5s 33ms/step - loss: 8.1229e-04\n",
      "Epoch 43/100\n",
      "145/145 [==============================] - 5s 31ms/step - loss: 7.8382e-04\n",
      "Epoch 44/100\n",
      "145/145 [==============================] - 5s 31ms/step - loss: 7.0778e-04\n",
      "Epoch 45/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 6.8534e-04\n",
      "Epoch 46/100\n",
      "145/145 [==============================] - 5s 34ms/step - loss: 7.2282e-04\n",
      "Epoch 47/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 6.8957e-04\n",
      "Epoch 48/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 7.0439e-04\n",
      "Epoch 49/100\n",
      "145/145 [==============================] - 5s 37ms/step - loss: 6.5244e-04\n",
      "Epoch 50/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 6.6467e-04\n",
      "Epoch 51/100\n",
      "145/145 [==============================] - 5s 36ms/step - loss: 6.5444e-04\n",
      "Epoch 52/100\n",
      "145/145 [==============================] - 5s 34ms/step - loss: 6.5099e-04\n",
      "Epoch 53/100\n",
      "145/145 [==============================] - 4s 31ms/step - loss: 7.0293e-04\n",
      "Epoch 54/100\n",
      "145/145 [==============================] - 5s 31ms/step - loss: 7.3923e-04\n",
      "Epoch 55/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 7.1253e-04\n",
      "Epoch 56/100\n",
      "145/145 [==============================] - 5s 32ms/step - loss: 7.0117e-04\n",
      "Epoch 57/100\n",
      "145/145 [==============================] - 5s 34ms/step - loss: 6.9427e-04\n",
      "Epoch 58/100\n",
      " 94/145 [==================>...........] - ETA: 1s - loss: 7.5120e-04"
     ]
    }
   ],
   "source": [
    "# Part 2 - Building the RNN\n",
    "# Building a robust stacked LSTM with dropout regularization\n",
    "\n",
    "# Initialising the RNN\n",
    "# Regression is when you predict a continuous value\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "# 'units' is the number of LSTM Memory Cells (Neurons) for higher dimensionality\n",
    "# 'return_sequences = True' because we will add more stacked LSTM Layers\n",
    "# 'input_shape' of x_train\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "# 20% of Neurons will be ignored (10 out of 50 Neurons) to prevent Overfitting\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "# Not need to specify input_shape for second Layer, it knows that we have 50 Neurons from the previous layer\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "# This is the last LSTM Layer. 'return_sequences = false' by default so we leave it out.\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "# 'units = 1' because Output layer has one dimension\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "# Keras documentation recommends 'RMSprop' as a good optimizer for RNNs\n",
    "# Trial and error suggests that 'adam' optimizer is a good choice\n",
    "# loss = 'mean_squared_error' which is good for Regression vs. 'Binary Cross Entropy' previously used for Classification\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "# 'X_train' Independent variables\n",
    "# 'y_train' Output Truths that we compare X_train to.\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 - Making the predictions and visualising the results\n",
    "\n",
    "# Getting the real stock price\n",
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "#display(dataset_test)\n",
    "# Getting the predicted stock price \n",
    "# We need 60 previous inputs for each day of the Test_set \n",
    "# Combine 'dataset_train' and 'dataset_test'\n",
    "# 'axis = 0' for Vertical Concatenation to add rows to the bottom\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n",
    "# Extract Stock Prices for Test time period, plus 60 days previous\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
    "# 'reshape' function to get it into a NumPy format\n",
    "inputs = inputs.reshape(-1,1)\n",
    "# Inputs need to be scaled to match the model trained on Scaled Feature\n",
    "inputs = sc.transform(inputs)\n",
    "# The following is pasted from above and modified for Testing, romove all 'Ys'\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# 'rg' is the length of the training data.  this ensures we loop through all of the training data\n",
    "rg = inputs.shape[0]\n",
    "\n",
    "for i in range(60, rg):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "    y_test.append(inputs[i, 0])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "# We need a 3D input so add another dimension\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "# Predict the Stock Price\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "# We need to inverse the scaling of our prediction to get a Dollar amount\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "\n",
    "y_pred = []\n",
    "rg = len(predicted_stock_price)\n",
    "for i in range(0, rg):    \n",
    "    y_pred.append(predicted_stock_price[i,0])\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "y_test = sc.inverse_transform(y_test)\n",
    "#display(y_test)\n",
    "#display(y_pred)\n",
    "\n",
    "rms = metrics.mean_squared_error(y_test, predicted_stock_price , squared=False)\n",
    "mae = metrics.mean_absolute_error(y_test, predicted_stock_price)\n",
    "rse = metrics.r2_score(y_test, y_pred, multioutput='variance_weighted', force_finite=False)\n",
    "print(\"Mean Squared Error: \")\n",
    "print(rms)\n",
    "print(\"Mean Absolute Error: \")\n",
    "print(mae)\n",
    "print(\"R-Squared Score: \")\n",
    "print(rse)\n",
    "\n",
    "# Visualising the results\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real ' + ticker + ' Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted ' + ticker + ' Stock Price')\n",
    "plt.title(ticker + ' Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(ticker + ' Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "388px",
    "left": "877px",
    "right": "20px",
    "top": "120px",
    "width": "543px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
