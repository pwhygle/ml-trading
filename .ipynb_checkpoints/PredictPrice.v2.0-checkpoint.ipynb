{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f02b027-e40b-44f0-9e79-aee1f1684507",
   "metadata": {},
   "source": [
    "#### Adapted from: https://machinelearningknowledge.ai/keras-lstm-layer-explained-for-beginners-with-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb87b53-0562-48a4-b94b-24b6574ddfca",
   "metadata": {},
   "source": [
    "# Set the Ticker and Data Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca5ca63-8452-4672-aa62-0f4a0483b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"AMC\"\n",
    "data_point = \"Open\" \n",
    "training_start = 1000\n",
    "training_end = 800\n",
    "latest_data_count = 1000\n",
    "time_frame = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fba166-42c7-4e3b-a36b-f855e9d298d2",
   "metadata": {},
   "source": [
    "# Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0115beaa-cd69-48f7-9f2c-553b9c47f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.datasets import imdb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#from keras.layers import LSTM, embeddings, dense\n",
    "#from keras.preprocessing.sequence import pad_sequence\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44658201-a076-4c11-9013-82b237659d9e",
   "metadata": {},
   "source": [
    "# Load the Training Dataset\n",
    "### Get the last 'training_start' days of records, and then get the first 'training_end' of those days for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a177bd48-dc80-402c-a10d-6c181488df74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "file = \"C:\\\\Users\\\\WhyglePa\\\\source\\\\repos\\\\algo-trading\\\\src\\\\Python\\\\TDAmeritrade\\\\Notebook\\\\ML\\\\Data\\\\\" + ticker + \".csv\"\n",
    "df = pd.read_csv(file)\n",
    "dataset_train = df.tail(training_start).head(training_end)\n",
    "dataset_test = df.tail(latest_data_count)\n",
    "\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd3467c-3759-4125-b0b8-4de2080c5b2e",
   "metadata": {},
   "source": [
    "## Focus on 'Open' Stock Price Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cba864d-af08-4023-b311-7de9c9414527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1)\n"
     ]
    }
   ],
   "source": [
    "training_set = dataset_train.loc[:,data_point:data_point].values\n",
    "\n",
    "print(training_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b3279-0306-4148-8188-74f624cb4451",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Scaling\n",
    "To produce the best-optimized results with the models, we are required to scale the data. For this task, we are leveraging scikit-learn library’s minmax scaler for converting the input values between 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4614ccf0-5dc0-45e2-bc68-9806fb6dc5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1)\n"
     ]
    }
   ],
   "source": [
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "print(training_set_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94906e68-0de5-4eea-a6d5-81bc28082c1b",
   "metadata": {},
   "source": [
    "# Creating Data with Timesteps\n",
    "When we are working with LSTM’s, we need to keep the data in a specific format. Once the data is created in the form of 60 timesteps, we can then convert it into a NumPy array. Finally, the data is converted to a 3D dimension array, 'time_frame' number of timeframes, and also one feature at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da23e34-03b8-4ab9-a743-4985c3604c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(740, 60, 1)\n",
      "(740,)\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "rg = dataset_train.shape[0]\n",
    "for i in range(time_frame, rg):\n",
    "    X_train.append(training_set_scaled[i-time_frame:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "    \n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f9354-79f4-4c9c-9af2-e7fff1aa158e",
   "metadata": {},
   "source": [
    "# Building the LSTM in Keras\n",
    "First, we add the Keras LSTM layer, and following this, we add dropout layers for prevention against overfitting.\n",
    "\n",
    "For the LSTM layer, we add 50 units that represent the dimensionality of outer space. The return_sequences parameter is set to true for returning the last output in output.\n",
    "\n",
    "For adding dropout layers, we specify the percentage of layers that should be dropped. The next step is to add the dense layer. At last, we compile the model with the help of adam optimizer. The error is computed using mean_squared_error.\n",
    "\n",
    "Finally, the model is fit using 100 epochs with a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0c305-1a75-4f45-9917-38f1d5ad1652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 [==============================] - 13s 67ms/step - loss: 0.0514\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0107\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0077\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0069\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0051\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0058\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 2s 67ms/step - loss: 0.0047\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 2s 67ms/step - loss: 0.0042\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0042\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0039\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0038\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 2s 67ms/step - loss: 0.0041\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 0.0039\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 0.0037\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0038\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0034\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0028\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0038\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0032\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 0.0027\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0025\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 2s 66ms/step - loss: 0.0024\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 0.0025\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 0.0020\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 0.0024\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 0.0026\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 2s 64ms/step - loss: 0.0026\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 2s 65ms/step - loss: 0.0023\n",
      "Epoch 29/100\n",
      " 6/24 [======>.......................] - ETA: 1s - loss: 0.0015"
     ]
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 256, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 256, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 256, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 256))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "#scores = regressor.evaluate(X_test, y_test, verbose=0)\n",
    "#print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab775386-de61-4c57-95e2-801b0f22ff42",
   "metadata": {},
   "source": [
    "# Predicting Future Stock using the Test Set\n",
    "For predicting the stock prices, next the training set and test set should be merged. The timestep is set to time_frame, we also apply MinMaxScaler on the new dataset and lastly, the dataset is reshaped.\n",
    "\n",
    "We are required to use inverse_transform for obtaining the stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f6b0f7-2e08-47db-b941-171c26a62007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "real_stock_price = dataset_test.loc[:,data_point:data_point].values\n",
    "\n",
    "dataset_total = pd.concat((dataset_train[data_point], dataset_test[data_point]), axis = 0)\n",
    "totLen = len(dataset_total) - len(dataset_test) - time_frame\n",
    "inputs = dataset_total.iloc[totLen:].values\n",
    "\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "\n",
    "rg = inputs.shape[0]\n",
    "\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(time_frame, rg):\n",
    "    X_test.append(inputs[i-time_frame:i, 0])\n",
    "    y_test.append(inputs[i, 0])\n",
    "    \n",
    "##X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=5000)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "#print(predicted_stock_price.shape)\n",
    "#print(X_test.shape)\n",
    "\n",
    "rms = mean_squared_error(y_test, predicted_stock_price , squared=False)\n",
    "mae = metrics.mean_absolute_error(y_test, predicted_stock_price)\n",
    "print(\"Mean Squared Error: \")\n",
    "print(rms)\n",
    "print(\"Mean Absolute Error: \")\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c6b17-dc7d-4a10-b89e-6af4bf5a7740",
   "metadata": {},
   "source": [
    "# Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7399e5b-b779-4873-ad0b-11ff8675d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(real_stock_price, color = 'red', label = ticker + ' Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted ' + ticker + ' Stock Price')\n",
    "plt.title(ticker + ' Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(ticker + ' Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec82c9-6be6-47f3-b7ca-b7606e601521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
