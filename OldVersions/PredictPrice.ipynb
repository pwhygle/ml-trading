{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f02b027-e40b-44f0-9e79-aee1f1684507",
   "metadata": {},
   "source": [
    "#### Adapted from: https://machinelearningknowledge.ai/keras-lstm-layer-explained-for-beginners-with-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb87b53-0562-48a4-b94b-24b6574ddfca",
   "metadata": {},
   "source": [
    "# Set the Ticker and Data Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca5ca63-8452-4672-aa62-0f4a0483b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"GME\"\n",
    "data_point = \"Open\"   # to do - use this everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fba166-42c7-4e3b-a36b-f855e9d298d2",
   "metadata": {},
   "source": [
    "# Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0115beaa-cd69-48f7-9f2c-553b9c47f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44658201-a076-4c11-9013-82b237659d9e",
   "metadata": {},
   "source": [
    "# Load the Training Dataset\n",
    "### Get the last 1000 days of records, and then get the first 500 of those days for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a177bd48-dc80-402c-a10d-6c181488df74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "file = \"C:\\\\Users\\\\WhyglePa\\\\source\\\\repos\\\\algo-trading\\\\src\\\\Python\\\\TDAmeritrade\\\\Notebook\\\\ML\\\\Data\\\\\" + ticker + \".csv\"\n",
    "dataset_train = pd.read_csv(file).tail(1000).head(500)\n",
    "print(len(dataset_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd3467c-3759-4125-b0b8-4de2080c5b2e",
   "metadata": {},
   "source": [
    "## Focus on 'Open' Stock Price Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cba864d-af08-4023-b311-7de9c9414527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1)\n"
     ]
    }
   ],
   "source": [
    "training_set = dataset_train.loc[:,data_point:data_point].values\n",
    "\n",
    "print(training_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653b3279-0306-4148-8188-74f624cb4451",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Scaling\n",
    "To produce the best-optimized results with the models, we are required to scale the data. For this task, we are leveraging scikit-learn library’s minmax scaler for converting the input values between 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4614ccf0-5dc0-45e2-bc68-9806fb6dc5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1)\n"
     ]
    }
   ],
   "source": [
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "print(training_set_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94906e68-0de5-4eea-a6d5-81bc28082c1b",
   "metadata": {},
   "source": [
    "# Creating Data with Timesteps\n",
    "When we are working with LSTM’s, we need to keep the data in a specific format. Once the data is created in the form of 60 timesteps, we can then convert it into a NumPy array. Finally, the data is converted to a 3D dimension array, 60 timeframes, and also one feature at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da23e34-03b8-4ab9-a743-4985c3604c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440, 60, 1)\n",
      "(440,)\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "rg = dataset_train.shape[0]\n",
    "for i in range(60, rg):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "    \n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f9354-79f4-4c9c-9af2-e7fff1aa158e",
   "metadata": {},
   "source": [
    "# Building the LSTM in Keras\n",
    "First, we add the Keras LSTM layer, and following this, we add dropout layers for prevention against overfitting.\n",
    "\n",
    "For the LSTM layer, we add 50 units that represent the dimensionality of outer space. The return_sequences parameter is set to true for returning the last output in output.\n",
    "\n",
    "For adding dropout layers, we specify the percentage of layers that should be dropped. The next step is to add the dense layer. At last, we compile the model with the help of adam optimizer. The error is computed using mean_squared_error.\n",
    "\n",
    "Finally, the model is fit using 100 epochs with a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0c305-1a75-4f45-9917-38f1d5ad1652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 [==============================] - 10s 56ms/step - loss: 0.0381\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.0126\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0090\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.0075\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.0065\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.0066\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.0076\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.0073\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.0062\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.0058\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0056\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.0052\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.0049\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.0057\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0047\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.0044\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0043\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.0043\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.0047\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.0044\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0055\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.0049\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0046\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0042\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.0043\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 0s 36ms/step - loss: 0.0043\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0043\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.0041\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0039\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.0040\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.0039\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0039\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0050\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.0043\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0041\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.0039\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.0035\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.0035\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0031\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.0034\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0038\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.0036\n",
      "Epoch 43/100\n",
      "13/14 [==========================>...] - ETA: 0s - loss: 0.0039"
     ]
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.1))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.1))\n",
    "\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.1))\n",
    "\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab775386-de61-4c57-95e2-801b0f22ff42",
   "metadata": {},
   "source": [
    "# Predicting Future Stock using the Test Set\n",
    "Read the file again, this time getting the last 1000 days of records.\n",
    "\n",
    "For predicting the stock prices, next the training set and test set should be merged. The timestep is set to 60, we also apply MinMaxScaler on the new dataset and lastly, the dataset is reshaped.\n",
    "\n",
    "We are required to use inverse_transform for obtaining the stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f6b0f7-2e08-47db-b941-171c26a62007",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = pd.read_csv(\"C:\\\\Users\\\\WhyglePa\\\\source\\\\repos\\\\algo-trading\\\\src\\\\Python\\\\TDAmeritrade\\\\Notebook\\\\ML\\\\Data\\\\\" + ticker + \".csv\").tail(1000)\n",
    "real_stock_price = dataset_test.loc[:,data_point:data_point].values\n",
    "\n",
    "\n",
    "dataset_total = pd.concat((dataset_train[data_point], dataset_test[data_point]), axis = 0)\n",
    "totLen = len(dataset_total) - len(dataset_test) - 60\n",
    "inputs = dataset_total.iloc[totLen:].values\n",
    "\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "\n",
    "rg = inputs.shape[0]\n",
    "\n",
    "\n",
    "X_test = []\n",
    "for i in range(60, rg):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c6b17-dc7d-4a10-b89e-6af4bf5a7740",
   "metadata": {},
   "source": [
    "# Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7399e5b-b779-4873-ad0b-11ff8675d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(real_stock_price, color = 'red', label = ticker + ' Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted ' + ticker + ' Stock Price')\n",
    "plt.title(ticker + ' Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(ticker + ' Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a11eb8-132d-4cfd-96a6-acc6ffc5e2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
